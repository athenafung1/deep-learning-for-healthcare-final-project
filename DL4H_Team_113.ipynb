{"cells":[{"cell_type":"markdown","metadata":{"id":"j01aH0PR4Sg-"},"source":["# FAQ and Attentions\n","* Copy and move this template to your Google Drive. Name your notebook by your team ID (upper-left corner). Don't eidt this original file.\n","* This template covers most questions we want to ask about your reproduction experiment. You don't need to exactly follow the template, however, you should address the questions. Please feel free to customize your report accordingly.\n","* any report must have run-able codes and necessary annotations (in text and code comments).\n","* The notebook is like a demo and only uses small-size data (a subset of original data or processed data), the entire runtime of the notebook including data reading, data process, model training, printing, figure plotting, etc,\n","must be within 8 min, otherwise, you may get penalty on the grade.\n","  * If the raw dataset is too large to be loaded  you can select a subset of data and pre-process the data, then, upload the subset or processed data to Google Drive and load them in this notebook.\n","  * If the whole training is too long to run, you can only set the number of training epoch to a small number, e.g., 3, just show that the training is runable.\n","  * For results model validation, you can train the model outside this notebook in advance, then, load pretrained model and use it for validation (display the figures, print the metrics).\n","* The post-process is important! For post-process of the results,please use plots/figures. The code to summarize results and plot figures may be tedious, however, it won't be waste of time since these figures can be used for presentation. While plotting in code, the figures should have titles or captions if necessary (e.g., title your figure with \"Figure 1. xxxx\")\n","* There is not page limit to your notebook report, you can also use separate notebooks for the report, just make sure your grader can access and run/test them.\n","* If you use outside resources, please refer them (in any formats). Include the links to the resources if necessary."]},{"cell_type":"markdown","metadata":{"id":"MQ0sNuMePBXx"},"source":["# Introduction\n","\n","## Background\n","\n","  Adverse drug-drug interaction (DDI) is the unintended molecular interactions between drugs. It’s a prominent cause of patient morbidities/mortalities, incurring higher costs and risking patient safety. The difficulty of mitigating this issue stems from a couple of factors: \n","\n","  * The molecular structure of drugs are complex, consisting of many units and substructures\n","  * Drug development is a process that requires highly specialized knowledge\n","  * Trials to test drugs and post-market surveillance are long and expensive processes\n","\n","  With respect to applying ML to this topic, there are also a couple of issues:\n","  \n","  * There is a relatively light amount of training data that exists, due to the slow reporting of DDI instances\n","  * Deep learning models have a large number of parameters, making interpretation of the model’s results difficult. Thus, extracting the reasoning for why a DDI is occurring can be hard for researchers to glean.\n","  * DDIs usually result from the reactions of only a few sub-structures of a drug’s entire molecule, but many drug-drug pairs have significant overlaps on larger but irrelevant substructures. This skews the results of DDI prediction.\n","\n","  There is major interest in predicting whether two drugs will interact (especially during the design process) to reduce testing/development costs and improving patient safety.\n","\n","### The Current State of the Art\n"," \n","  Deep learning models have been successfully used to predict DDIs, however such previous models often generate drug representations using the entire chemical representation, causing learned representations to be potentially biased toward the large, irrelevant substructures and ultimately nullify learned drug similarities and predictions.\n","\n","## CASTER\n","  The ChemicAl SubstrucTurE Representation framework, or CASTER, was introduced as a DDI prediction model, improving on the weaknesses of prior works.\n","  * what did the paper propose\n","  * what is the innovations of the method\n","  * how well the proposed method work (in its own metrics)\n","  * what is the contribution to the reasearch regime (referring the Background above, how important the paper is to the problem).\n"]},{"cell_type":"markdown","metadata":{"id":"uygL9tTPSVHB"},"source":["# Scope of Reproducibility:\n","\n","The authors of CASTER presented the following hypotheses:\n","\n","1.  The CASTER model will provide more accurate DDI predictions when compared with other established models.\n","\n","2.  The usage of unlabelled data to generate frequent sub-structure features improves performance in situations with limited labeled datasets.\n","\n","3.  CASTER’s sub-structure dictionary can help human operators better comprehend the final result.\n"]},{"cell_type":"markdown","metadata":{"id":"xWAHJ_1CdtaA"},"source":["# Methodology"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"yu61Jp1xrnKk"},"outputs":[],"source":["#Just a bunch of imports\n","\n","import torch\n","from torch.autograd import Variable\n","import torch.nn.functional as F\n","from torch.utils import data\n","from torch import nn \n","import copy\n","\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","from time import time\n","from sklearn.metrics import roc_auc_score, precision_recall_curve\n","from sklearn.model_selection import KFold\n","torch.manual_seed(2)    # reproducible torch:2 np:3\n","np.random.seed(3)\n","\n","#Used for parsing the CASTER dataset\n","from subword_nmt.apply_bpe import BPE\n","import codecs\n","\n"]},{"cell_type":"markdown","metadata":{"id":"2NbPHUTMbkD3"},"source":["##  Data\n","The data that the model ingests consists of 3 datasets:\n","  * unsup_dataset.csv = A dataset of randomly combined pairs of SMILEs strings drawn from FooDB (a db of food constituent molecules) and all drugs, drawn from DrugBank\n","    * This is the unsupervised dataset used to help find frequent SMILEs substructures\n","  * BIOSNAP/sup*.csv = A dataset from Stanford's Biomedical Network Dataset indicating pairs of SMILEs strings and presence of DDI\n","    * These are the supervised datasets\n","  * subword_units_map.csv = The 1722 frequent patterns extracted from the above strings, produced by the sequential pattern mining (SPM) routine described in CASTER\n","  \n","Data includes raw data (MIMIC III tables), descriptive statistics (our homework questions), and data processing (feature engineering).\n","  * Source of the data: where the data is collected from; if data is synthetic or self-generated, explain how. If possible, please provide a link to the raw datasets.\n","  * Statistics: include basic descriptive statistics of the dataset like size, cross validation split, label distribution, etc.\n","  * Data process: how do you munipulate the data, e.g., change the class labels, split the dataset to train/valid/test, refining the dataset.\n","  * Illustration: printing results, plotting figures for illustration.\n","  * You can upload your raw dataset to Google Drive and mount this Colab to the same directory. If your raw dataset is too large, you can upload the processed dataset and have a code to load the processed dataset."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["#===== DATASET DEFINITIONS =====\n","\n","dataFolder = './data'\n","\n","vocab_path = dataFolder + '/codes.txt'\n","bpe_codes_fin = codecs.open(vocab_path)\n","bpe = BPE(bpe_codes_fin, merges=-1, separator='')\n","\n","#Get frequent substructures\n","vocab_map = pd.read_csv(dataFolder + '/subword_units_map.csv')\n","idx2word = vocab_map['index'].values\n","words2idx = dict(zip(idx2word, range(0, len(idx2word))))\n","\n","#===== Helper functions =====\n","#Map the smiles strings into multi-hot representations of substructures.\n","def smiles2index(s1, s2):\n","    t1 = bpe.process_line(s1).split() #split\n","    t2 = bpe.process_line(s2).split() #split\n","    i1 = [words2idx[i] for i in t1] # index\n","    i2 = [words2idx[i] for i in t2] # index\n","    return i1, i2\n","\n","#Combine both multi-hot representations into a single multi-hot (the functional representation)\n","def index2multi_hot(i1, i2):\n","    v_d = np.zeros(len(idx2word),)\n","    v_d[i1] = 1\n","    v_d[i2] = 1\n","    return v_d\n","\n","#Final product, takes two smiles strings and turns them into the functional representation multi-hot.\n","def smiles2vector(s1, s2):\n","    i1, i2 = smiles2index(s1, s2)\n","    v_d = index2multi_hot(i1, i2)\n","    return v_d\n","\n","#===== Datasets =====\n","class sup_data(data.Dataset):\n","\n","    def __init__(self, list_IDs, labels, df_ddi):\n","        'Initialization'\n","        self.labels = labels\n","        self.list_IDs = list_IDs\n","        self.df = df_ddi\n","        \n","    def __len__(self):\n","        'Denotes the total number of samples'\n","        return len(self.list_IDs)\n","\n","    def __getitem__(self, index):\n","        'Generates one sample of data'\n","        # Select sample\n","        index = self.list_IDs[index]\n","        # Load data from sample and get multi-hot\n","        s1 = self.df.iloc[index].Drug1_SMILES\n","        s2 = self.df.iloc[index].Drug2_SMILES\n","        v_d = smiles2vector(s1, s2)\n","        #Get label\n","        y = self.labels[index]\n","        #Return combined multi-hot and its label.\n","        return v_d, y\n","    \n","class unsup_data(data.Dataset):\n","\n","    def __init__(self, list_IDs, df):\n","        'Initialization'\n","        self.list_IDs = list_IDs\n","        self.df = df\n","\n","    def __len__(self):\n","        'Denotes the total number of samples'\n","        return len(self.list_IDs)\n","\n","    def __getitem__(self, index):\n","        'Generates one sample of data'\n","        # Load data and get label\n","        index = self.list_IDs[index]\n","        s1 = self.df.iloc[index].input1_SMILES\n","        s2 = self.df.iloc[index].input2_SMILES\n","        v_d = smiles2vector(s1, s2)\n","        return v_d"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"BZScZNbROw-N"},"outputs":[{"name":"stdout","output_type":"stream","text":["==============================\n","TOTAL UNSUPERVISED SET\n","==============================\n","# of samples: 441853\n","# of drug-food pairs: 220926\n","# of drug-drug pairs: 220927\n","==============================\n","BIOSNAP TRAIN/VAL SET\n","==============================\n","# of samples: 66432\n","# of DDIs: 33243\n","1722\n"]}],"source":["df_unsup = pd.read_csv('data/unsup_dataset.csv', names = ['idx', 'input1_SMILES', 'input2_SMILES', 'type']).drop(0)# pairs dataframe input1_smiles, input2_smiles\n","df_ddi = pd.read_csv('data/BIOSNAP/sup_train_val.csv')  # ddi dataframe drug1_smiles, drug2_smiles\n","\n","#Print some basic info about the datasets \n","print(\"=\"*30)\n","print(\"TOTAL UNSUPERVISED SET\")\n","print(\"=\"*30)\n","print(\"# of samples:\",len(df_unsup))\n","print(\"# of drug-food pairs:\",sum(df_unsup.type == \"df_pair\"))\n","print(\"# of drug-drug pairs:\",sum(df_unsup.type == \"dd_pair\"))\n","print(\"=\"*30)\n","print(\"BIOSNAP TRAIN/VAL SET\")\n","print(\"=\"*30)\n","print(\"# of samples:\",len(df_ddi))\n","print(\"# of DDIs:\",sum(df_ddi.label == 1.0))\n","\n","#Do kfold\n","kf = KFold(n_splits = 8, shuffle = True, random_state = 3)\n","\n","#Get the 1st fold index\n","fold_index = next(kf.split(df_ddi), None)\n","\n","ids_unsup = df_unsup.index.values\n","partition_sup = {'train': fold_index[0], 'val': fold_index[1]}\n","labels_sup = df_ddi.label.values\n","\n","LOADER_PARAMS = {\n","    'batch_size': 128,\n","    'shuffle': True,\n","    'num_workers': 0\n","    }\n","\n","unsup_set = unsup_data(ids_unsup, df_unsup)\n","unsup_generator = data.DataLoader(unsup_set, **LOADER_PARAMS)\n","\n","training_set = sup_data(partition_sup['train'], labels_sup, df_ddi)\n","training_generator_sup = data.DataLoader(training_set, **LOADER_PARAMS)\n","\n","validation_set = sup_data(partition_sup['val'], labels_sup, df_ddi)\n","validation_generator_sup = data.DataLoader(validation_set, **LOADER_PARAMS)\n","print(len(training_set.__getitem__(0)[0]))"]},{"cell_type":"markdown","metadata":{"id":"3muyDPFPbozY"},"source":["##   Model\n","\n","![caster_structure.png](img/caster_block_diagram.png)\n","\n","### Architecture\n","The model consists of a few notable parts:\n","* Sequential Pattern Mining (SPM)\n","    * The SMILEs strings are initially passed through a sequential pattern mining process to find frequently occurring substructures\n","    * SMILEs strings are encoded into multi-hot vectors of frequent substructures and combined into a single functional representation (X)\n","* Encoder\n","    * The functional representation is passed through the encoder to make a latent feature vector (z)\n","    * The individual freq. substructures, as one-hot vectors, are also passed through the encoder to make a latent dictionary of substructures (b_1-k)\n","        * Note: This is equivalent to passing an identity matrix through the encoder.\n","* Predictor\n","    * The coefficients are passed through a standard fully connected NN.\n","    \n","\n","The model includes the model definition which usually is a class, model training, and other necessary parts.\n","* Model architecture: layer number/size/type, activation function, etc\n","* Training objectives: loss function, optimizer, weight of each loss term, etc\n","* Others: whether the model is pretrained, Monte Carlo simulation for uncertainty analysis, etc\n","* The code of model should have classes of the model, functions of model training, model validation, etc.\n","* If your model training is done outside of this notebook, please upload the trained model here and develop a function to load and test it."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#This defines the configuration of the model/training.\n","CONFIG = {}\n","\n","CONFIG['batch_size'] = 256\n","\n","#The width of the drug-drug pair's multi-hot functional representation\n","CONFIG['input_dim'] = 1722\n","CONFIG['batch_first'] = True\n","CONFIG['num_class'] = 2\n","\n","#Learning rate\n","CONFIG['LR'] = 1e-3\n","CONFIG['train_epoch'] = 3\n","CONFIG['pretrain_epoch'] = 1\n","\n","CONFIG['recon_threshold'] = 0.0005 # change later\n","\n","#===== Encoder/Decoder Parameters =====\n","CONFIG['encode_fc1_dim'] = 500  # encoder fc1\n","CONFIG['encode_fc2_dim'] = 50  # encoder fc2\n","CONFIG['decode_fc1_dim'] = 500  # decoder fc1\n","CONFIG['decode_fc2_dim'] = CONFIG['input_dim']  # decoder reconstruction\n","\n","#===== Deep Predictor Parameters =====\n","CONFIG['predict_dim'] = 1024 # for every layer\n","CONFIG['predict_out_dim'] = 1 # predictor out\n","\n","CONFIG['lambda1'] = 1e-2  # L1 regularization coefficient\n","CONFIG['lambda2'] = 1e-1  # L2 regulatization coefficient\n","CONFIG['lambda3'] = 1e-5  # L2 regulatization coefficient\n","\n","CONFIG['reconstruction_coefficient'] = 1e-1  # 1e-2\n","CONFIG['projection_coefficient'] = 1e-1  # 1e-2\n","CONFIG['magnify_factor'] = 100"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"gBdVZoTvsSFV"},"outputs":[{"ename":"NameError","evalue":"name 'nn' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCASTER\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mSequential):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;28msuper\u001b[39m(CASTER, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n","\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"]}],"source":["class CASTER(nn.Sequential):\n","\n","    def __init__(self, **config):\n","        super(CASTER, self).__init__()\n","        self.input_dim = config['input_dim']\n","        self.num_class = config['num_class']\n","        self.lambda3 = config['lambda3']        \n","        self.encode_fc1_dim = config['encode_fc1_dim']\n","        self.encode_fc2_dim = config['encode_fc2_dim']\n","        self.decode_fc1_dim = config['decode_fc1_dim']\n","        self.decode_fc2_dim = config['decode_fc2_dim']\n","        self.predict_dim = config['predict_dim']\n","        self.predict_out_dim = config['predict_out_dim']\n","        self.mag_factor = config['magnify_factor']        \n","\n","        # encoder: two layer NN\n","        self.encoder = nn.Sequential(\n","            nn.Linear(self.input_dim, self.encode_fc1_dim),\n","            nn.ReLU(True),\n","            nn.Linear(self.encode_fc1_dim, self.encode_fc2_dim)\n","        )\n","        # decoder: two layer NN\n","        self.decoder = nn.Sequential(\n","            nn.Linear(self.encode_fc2_dim, self.decode_fc1_dim),\n","            nn.ReLU(True),\n","            nn.Linear(self.decode_fc1_dim, self.decode_fc2_dim)\n","        )\n","\n","        # predictor: eight layer NN\n","        self.predictor = nn.Sequential(\n","            # layer 1\n","            nn.Linear(self.input_dim, self.predict_dim),\n","            nn.ReLU(True),\n","            # layer 2\n","            nn.BatchNorm1d(self.predict_dim),\n","            nn.Linear(self.predict_dim, self.predict_dim),\n","            nn.ReLU(True),\n","            # layer 3\n","            nn.BatchNorm1d(self.predict_dim),\n","            nn.Linear(self.predict_dim, self.predict_dim),\n","            nn.ReLU(True),\n","            # layer 4\n","            nn.BatchNorm1d(self.predict_dim),\n","            nn.Linear(self.predict_dim, self.predict_dim),\n","            nn.ReLU(True),\n","            # layer 5\n","            nn.BatchNorm1d(self.predict_dim),\n","            nn.Linear(self.predict_dim, self.predict_dim),\n","            nn.ReLU(True),\n","            # layer 6\n","            nn.BatchNorm1d(self.predict_dim),\n","            nn.Linear(self.predict_dim, 64),\n","            nn.ReLU(True),\n","            # output layer\n","            nn.Linear(64, self.predict_out_dim)\n","        )\n","\n","def dictionary_encoder(self, Z_D, Z_f):\n","    '''\n","    :param Z_D: batch_size x encode_fc2_dim\n","    :param Z_f: encode_fc2_dim x eta\n","    :return: sparse code X_o: batch_size x eta\n","    '''       \n","    \n","    DTD = torch.matmul(Z_f, Z_f.transpose(2, 1))  # D is Dictionary;  D^T D encode_dim x eta\n","    DTD_inv = torch.inverse(DTD + self.lambda3 * torch.eye(self.input_dim))  # (D^T D + \\lambda2 I )^{-1} D^T D, eta x eta\n","    DTD_inv_DT = torch.matmul(DTD_inv, Z_f)  \n","\n","    r = Z_D[:,None,:].matmul(DTD_inv_DT.transpose(2, 1)).squeeze(1) # batch_size x eta    \n","    return r\n","\n","def forward(self, x_of_pair):\n","    '''\n","    :param x_of_pair: batch_size x width of multi-hot functional\n","    :return: recon, score, code\n","    '''\n","    _, x_width = x_of_pair.shape\n","    \n","    # Encode functional representation into latent representation \n","    z_of_pair = self.encoder(x_of_pair)\n","    \n","    # Create latent dictionary\n","    b = self.encoder(torch.eye(x_width))\n","    \n","    b = b.mul(x_of_pair[:,:,None]) \n","\n","    # Decode latent representation\n","    recon_temp = self.decoder(z_of_pair)\n","    reconstructed = torch.sigmoid(recon_temp)\n","    \n","    # dictionary learning\n","    code = self.dictionary_encoder(z_of_pair, b)\n","\n","    # Pass coeffs through the deep predictor\n","    score = self.predictor(self.mag_factor * code)\n","    return reconstructed, code, score, b, z_of_pair\n","\n","#Instantiate Model\n","model = CASTER(CONFIG)\n","loss_func = None\n","log_reg_optimizer = None\n","\n","def train_model_one_iter(model, loss_func, optimizer):\n","    pass\n","\n","num_epoch = 10\n","# model training loop: it is better to print the training/validation losses during the training\n","for i in range(num_epoch):\n","    train_model_one_iter(model, loss_func, log_reg_optimizer)\n","    train_loss, valid_loss = None, None\n","    print(\"Train Loss: %.2f, Validation Loss: %.2f\" % (train_loss, valid_loss))"]},{"cell_type":"markdown","metadata":{},"source":["## Ablation Model: Logistic Regression\n","\n","One of the ablations we proposed (and one that was also suggested in the paper) was using logistic regression (LR) instead of CASTER's deep predictor. Specifically, the pair's functional representation (sub-structured/post pattern-mined) is classified using LR. According to the paper, though far lighter in parameter count, it was not chosen due its weaker performance."]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Starting training\n","Epoch: 0. Loss: 0.6749600020899327. Accuracy: 0\n","Epoch: 1. Loss: 0.6640584605572155. Accuracy: 0\n","Epoch: 2. Loss: 0.648485678074174. Accuracy: 0\n","Epoch: 3. Loss: 0.7013468194961268. Accuracy: 0\n","Epoch: 4. Loss: 0.6657262232705916. Accuracy: 0\n","Epoch: 5. Loss: 0.6544603837170421. Accuracy: 0\n","Epoch: 6. Loss: 0.6249702524725425. Accuracy: 0\n","Epoch: 7. Loss: 0.6263930347633465. Accuracy: 0\n","Epoch: 8. Loss: 0.6580098734968453. Accuracy: 0\n","Epoch: 9. Loss: 0.6833581732996595. Accuracy: 0\n","Epoch: 10. Loss: 0.6631222575097829. Accuracy: 0\n","Epoch: 11. Loss: 0.6573572399505658. Accuracy: 0\n","Epoch: 12. Loss: 0.6518567596097673. Accuracy: 0\n","Epoch: 13. Loss: 0.6227891636092892. Accuracy: 0\n","Epoch: 14. Loss: 0.6992572614928463. Accuracy: 0\n","Epoch: 15. Loss: 0.6175400640164673. Accuracy: 0\n","Epoch: 16. Loss: 0.6332888308638058. Accuracy: 0\n","Epoch: 17. Loss: 0.632756286974647. Accuracy: 0\n","Epoch: 18. Loss: 0.5793076304849437. Accuracy: 0\n","Epoch: 19. Loss: 0.6134011414760278. Accuracy: 0\n","Epoch: 20. Loss: 0.6190628983958661. Accuracy: 0\n","Epoch: 21. Loss: 0.6190135392846127. Accuracy: 0\n","Epoch: 22. Loss: 0.6520505440291718. Accuracy: 0\n","Epoch: 23. Loss: 0.6877852800050964. Accuracy: 0\n","Epoch: 24. Loss: 0.6141231451468342. Accuracy: 0\n","Epoch: 25. Loss: 0.5512052470094517. Accuracy: 0\n","Epoch: 26. Loss: 0.5478879109500271. Accuracy: 0\n","Epoch: 27. Loss: 0.611452142744344. Accuracy: 0\n","Epoch: 28. Loss: 0.6354996871450815. Accuracy: 0\n","Epoch: 29. Loss: 0.6257822765679918. Accuracy: 0\n","Epoch: 30. Loss: 0.5793923940077047. Accuracy: 0\n","Epoch: 31. Loss: 0.5812944001854567. Accuracy: 0\n","Epoch: 32. Loss: 0.6398978058348972. Accuracy: 0\n","Epoch: 33. Loss: 0.656673775290999. Accuracy: 0\n","Epoch: 34. Loss: 0.5333307978740859. Accuracy: 0\n","Epoch: 35. Loss: 0.6080512516575314. Accuracy: 0\n","Epoch: 36. Loss: 0.5916829044101684. Accuracy: 0\n","Epoch: 37. Loss: 0.5537220770287263. Accuracy: 0\n","Epoch: 38. Loss: 0.6118828362175516. Accuracy: 0\n","Epoch: 39. Loss: 0.6412605680843158. Accuracy: 0\n","Epoch: 40. Loss: 0.7145535357793438. Accuracy: 0\n","Epoch: 41. Loss: 0.6149278765995705. Accuracy: 0\n","Epoch: 42. Loss: 0.6507955631175021. Accuracy: 0\n","Epoch: 43. Loss: 0.6510189366821014. Accuracy: 0\n","Epoch: 44. Loss: 0.5458474868044072. Accuracy: 0\n","Epoch: 45. Loss: 0.6259968500506795. Accuracy: 0\n","Epoch: 46. Loss: 0.4905584124046866. Accuracy: 0\n","Epoch: 47. Loss: 0.5789470187531277. Accuracy: 0\n","Epoch: 48. Loss: 0.6255445181182511. Accuracy: 0\n","Epoch: 49. Loss: 0.5190989155659853. Accuracy: 0\n"]}],"source":["class simple_log_reg(nn.Module):\n","\n","    def __init__(self, n_inputs, n_outputs):\n","        super(simple_log_reg, self).__init__()\n","        self.linear = torch.nn.Linear(n_inputs, n_outputs)\n","        self.double()\n","    \n","    def forward(self,x):\n","        pred = torch.sigmoid(self.linear(x))\n","        return pred[:,-1]\n","\n","n_inputs = CONFIG[\"input_dim\"] #Width of the multi-hot vector\n","n_outputs = 1\n","log_reg = simple_log_reg(n_inputs, n_outputs)\n","\n","# defining the optimizer\n","log_reg_optimizer = torch.optim.SGD(log_reg.parameters(), lr=0.001)\n","# defining Cross-Entropy loss\n","log_reg_criterion = torch.nn.BCELoss()\n","\n","epochs = 50\n","Loss = []\n","acc = []\n","print(\"Starting training\")\n","for epoch in range(epochs):\n","    # print(\"epoch:\",epoch)\n","    for x_of_pair, y in training_generator_sup:\n","        # print(y)\n","        log_reg_optimizer.zero_grad()\n","        # print(x)\n","        # print(x.shape)\n","        y_pred = log_reg(x_of_pair)\n","        loss = log_reg_criterion(y_pred, y)\n","        # Loss.append(loss.item())\n","        loss.backward()\n","        log_reg_optimizer.step()\n","    Loss.append(loss.item())\n","    correct = 0\n","    # for x, y in validation_generator_sup:\n","    #     y_pred = log_reg(x)\n","    #     # print(y_pred)\n","    #     predicted = y_pred > 0.5\n","    #     correct = (predicted == y).sum()\n","    # accuracy = 100 * (correct.item()) / len(y)\n","    # acc.append(accuracy)\n","    print('Epoch: {}. Loss: {}. Accuracy: {}'.format(epoch, loss.item(), 0))\n","print(\"Done training LR.\")"]},{"cell_type":"markdown","metadata":{"id":"gX6bCcZNuxmz"},"source":["# Results\n","In this section, you should finish training your model training or loading your trained model. That is a great experiment! You should share the results with others with necessary metrics and figures.\n","\n","Please test and report results for all experiments that you run with:\n","\n","*   specific numbers (accuracy, AUC, RMSE, etc)\n","*   figures (loss shrinkage, outputs from GAN, annotation or label of sample pictures, etc)\n","\n","To begin the evaluation process, we iterate through a test data set using a data loader to obtain the predictions (y_pred) from the model for each input (v_D). We then use the ground truth labels (y_label) to which we compare our predictions to calculate the evaluation metrics.\n","\n","The evaluation metrics used here include some of the most commonly used metrics to assess the performance of binary models: \n","\n","* **Average Precision Score**: Measures the precision-recall trade-off by summarizing the precision-recall curve as a weighted mean of precisions achieved at various thresholds.\n","\n","* **Area under the ROC Curve (AUC)**: This metric effectively describes the trade-off between true-positive rate and false-positive rate. It essentially quantifies the ability of the model to distingush between classes.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LjW9bCkouv8O"},"outputs":[],"source":["# metrics to evaluate my model\n","\n","# plot figures to better show the results\n","\n","# it is better to save the numbers and figures for your presentation.\n","\n","from matplotlib import pyplot as plt\n","params = {'batch_size': 256,\n","              'shuffle': True,\n","              'num_workers': 6}\n","\n","dataFolder = './data'\n","\n","df_ddi = pd.read_csv('../data/SNAP/sup_test.csv')  # ddi dataframe drug1_smiles, drug2_smiles\n","labels_sup = df_ddi.label.values\n","test_set = supData(df_ddi.index.values, labels_sup, df_ddi)\n","test_generator_sup = data.DataLoader(test_set, **params)\n","\n","model_nn = model_max\n","\n","y_pred = []\n","y_label = []\n","model_nn.eval()\n","for i, (v_D, label) in tqdm(enumerate(test_generator_sup)):\n","    recon, code, score, Z_f, z_D = model_nn(v_D.float())\n","    m = torch.nn.Sigmoid()\n","    logits = torch.squeeze(m(score)).detach().cpu().numpy()\n","    label_ids = label.to('cpu').numpy()\n","    y_label = y_label + label_ids.flatten().tolist()\n","    y_pred = y_pred + logits.flatten().tolist()"]},{"cell_type":"markdown","metadata":{},"source":["In the following cells, we will plot the precision-recall curve and the AUC described above. The precision-recall curve demonstrates the precision of the model at various recall levels. This gives us an idea of the model's ability to identify positive occurences with false positives minimized.\n","\n","Overall, using these metrics and evaluations, we get a clear picture of the model's performance across different thresholds."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.metrics import average_precision_score\n","average_precision_score(y_label, y_pred)\n","from sklearn.metrics import precision_recall_curve\n","from sklearn.utils.fixes import signature\n","average_precision = average_precision_score(y_label, y_pred)\n","precision, recall, _ = precision_recall_curve(y_label, y_pred)\n","\n","# In matplotlib < 1.5, plt.fill_between does not have a 'step' argument\n","step_kwargs = ({'step': 'post'}\n","               if 'step' in signature(plt.fill_between).parameters\n","               else {})\n","plt.step(recall, precision, color='b', alpha=0.2,\n","         where='post')\n","plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n","\n","plt.xlabel('Recall')\n","plt.ylabel('Precision')\n","plt.ylim([0.0, 1.05])\n","plt.xlim([0.0, 1.0])\n","plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(\n","          average_precision))\n","average_precision"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.metrics import roc_auc_score, precision_recall_curve, roc_curve, auc, confusion_matrix, classification_report\n","\n","fpr, tpr, thresholds = roc_curve(y_label, y_pred)\n","auc_score = auc(fpr, tpr)\n","\n","plt.figure(1)\n","plt.plot([0, 1], [0, 1], 'k--')\n","plt.plot(fpr, tpr, label='Val (area = {:.3f})'.format(auc_score))\n","plt.xlabel('False positive rate')\n","plt.ylabel('True positive rate')\n","plt.title('ROC curve')\n","plt.legend(loc='best')"]},{"cell_type":"markdown","metadata":{"id":"8EAWAy_LwHlV"},"source":["# Model Comparison"]},{"cell_type":"markdown","metadata":{"id":"qH75TNU71eRH"},"source":["# Discussion\n","\n","In this section,you should discuss your work and make future plan. The discussion should address the following questions:\n","  * Make assessment that the paper is reproducible or not.\n","  * Explain why it is not reproducible if your results are kind negative.\n","  * Describe “What was easy” and “What was difficult” during the reproduction.\n","  * Make suggestions to the author or other reproducers on how to improve the reproducibility.\n","  * What will you do in next phase.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E2VDXo5F4Frm"},"outputs":[],"source":["# no code is required for this section\n","'''\n","if you want to use an image outside this notebook for explanaition,\n","you can read and plot it here like the Scope of Reproducibility\n","'''"]},{"cell_type":"markdown","metadata":{"id":"SHMI2chl9omn"},"source":["# References\n","\n","1.   Huang, K., Xiao, C., Hoang, T., Glass, L., & Sun, J. (2020, April). Caster: Predicting drug interactions with chemical substructure representation. In Proceedings of the AAAI conference on artificial intelligence (Vol. 34, No. 01, pp. 702-709).\n","\n"]}],"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"1oAKqszNlwEZwPa_BjHPqfcoWlikYBpi5","timestamp":1709153069464}],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.2"}},"nbformat":4,"nbformat_minor":0}
