{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j01aH0PR4Sg-"
   },
   "source": [
    "# Github Link:\n",
    "https://github.com/athenafung1/deep-learning-for-healthcare-final-project\n",
    "\n",
    "# Video Link:\n",
    "**FILL IN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQ0sNuMePBXx"
   },
   "source": [
    "# Introduction:\n",
    "---\n",
    "\n",
    "## Background\n",
    "\n",
    "  Adverse drug-drug interaction (DDI) is the unintended molecular interactions between drugs. It’s a prominent cause of patient morbidities/mortalities, incurring higher costs and risking patient safety. The difficulty of mitigating this issue stems from a couple of factors: \n",
    "\n",
    "  * The molecular structure of drugs are complex, consisting of many units and substructures\n",
    "  * Drug development is a process that requires highly specialized knowledge\n",
    "  * Trials to test drugs and post-market surveillance are long, expensive processes\n",
    "\n",
    "  With respect to applying ML to this topic, there are also a couple of issues:\n",
    "  \n",
    "  * There is a relatively light amount of training data that exists, due to the slow reporting of DDI instances\n",
    "  * Deep learning models have a large number of parameters, making interpretation of the model’s results difficult. One example is that it can be hard to extract the reason for *why* a DDI is occurring.\n",
    "  * DDIs usually result from the reactions of only a few sub-structures of a drug’s entire molecule, but many drug-drug pairs have significant overlaps on larger but irrelevant substructures. This skews the results of DDI prediction.\n",
    "\n",
    "  There is major interest in predicting whether two drugs will interact (especially during the design process) to reduce testing/development costs and to improve patient safety.\n",
    "\n",
    "### The Current State of the Art\n",
    " \n",
    "  Deep learning models have been successfully used to predict DDIs, however such previous models often digest the entire chemical representation, causing learned representations to be potentially biased toward the large, irrelevant substructures and ultimately nullify learned drug similarities and predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CASTER\n",
    "---\n",
    "The **ChemicAl SubstrucTurE Representation** framework (CASTER) was introduced as a DDI prediction model, improving on the weaknesses of prior works. CASTER adds a couple of improvements upon previous works. It includes a sequential pattern mining (SPM) method to efficiently decompose a SMILES string into a set of functional drug sub-structures. This accounts for the interaction mechanism between drugs, as interactions depend primarily on the reaction between only a few functional sub-structures. It also includes an autoencoder that works in tandem with a dictionary learning module. This allows the decoding of the latent result, which can help human operators better understand the probability of which sub-structures of a molecule pair are interacting. \n",
    "\n",
    "The authors presented the following hypotheses:\n",
    "1.  The CASTER model will provide more accurate DDI predictions when compared with other established models.\n",
    "2.  The usage of unlabelled data to generate frequent sub-structure features improves performance in situations with limited labeled datasets.\n",
    "3.  CASTER’s sub-structure dictionary can help human operators better comprehend the final result.\n",
    "\n",
    "Based on the paper, it outperformed multiple other models while having a slightly smaller set of parameters, as shown below:\n",
    "\n",
    "![caster_results.png](img/caster_results.png)\n",
    "\n",
    "### Architecture\n",
    "\n",
    "![caster_structure.png](img/caster_block_diagram.png)\n",
    "\n",
    "The model consists of a few notable parts:\n",
    "* **Sequential Pattern Mining (SPM)**\n",
    "    * The SMILEs strings are initially passed through a sequential pattern mining process to find frequently occurring substructures\n",
    "        * The frequent substructures are put into a dictionary (*u[1-k]*)\n",
    "    * SMILEs pairs are encoded into multi-hot vectors of their frequent substructures and combined into a single functional representation (*X*)\n",
    "* **Encoder**\n",
    "    * The functional representation is passed through the encoder to make a latent feature vector (*z*)\n",
    "    * The individual freq. substructures, as one-hot vectors, are also passed through the encoder to make a latent dictionary of substructures (*b[1-k]*)\n",
    "* **Predictor**\n",
    "    * The latent representation is projected onto the latent dictionary to form a set of coefficients (*r[1-k]*)\n",
    "    * The coefficients are passed through a standard fully connected NN.\n",
    "* **Decoder**\n",
    "    * The latent representation is passed through the decoder to reconstruct the functional representation and help comprehend the result\n",
    "\n",
    "CASTER is noteworthy for its progress in considering the holistic structures of molecules, and the fact that it can provide a more understandable, practical output to a human operator using the tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uygL9tTPSVHB"
   },
   "source": [
    "## Scope of Reproducibility:\n",
    "\n",
    "We will attempt to test the two following hypotheses with the same hyperparameters defined in the paper/repo:\n",
    "\n",
    "1.  *The CASTER model will provide more accurate DDI predictions when compared with other established models.*\n",
    "2.  *The usage of unlabelled data to generate frequent sub-structure features improves performance in situations with limited labeled datasets.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWAHJ_1CdtaA"
   },
   "source": [
    "# Methodology:\n",
    "---\n",
    "### Setup:\n",
    "This notebook was run with Python >=3.8. The required packages below can be installed using conda, venv, or pip.\n",
    "* numpy\n",
    "* torch\n",
    "* subword_nmt\n",
    "* codecs\n",
    "* pandas\n",
    "* scikit-learn\n",
    "* tqdm\n",
    "* matplotlib\n",
    "\n",
    "### Running the Notebook:\n",
    "Hit `Run All`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "yu61Jp1xrnKk"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Standard torch imports\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "from torch import nn \n",
    "\n",
    "#Used for parsing the misc datasets\n",
    "from subword_nmt.apply_bpe import BPE\n",
    "import codecs\n",
    "\n",
    "#For dataset preparation and evaluation\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "#Misc.\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "torch.manual_seed(2)    # reproducible torch:2 np:3\n",
    "np.random.seed(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUICK_TRAIN_DEMO = True\n",
    "\n",
    "#Defines configuration for the data/model/training.\n",
    "CONFIG = {}\n",
    "\n",
    "#Learning rate\n",
    "CONFIG['LR'] = 1e-3\n",
    "CONFIG['batch_size'] = 128\n",
    "CONFIG['batch_first'] = True\n",
    "\n",
    "#Number of pretrain and training epochs\n",
    "CONFIG['pretrain_epochs'] = 3\n",
    "CONFIG['train_epochs'] = 3\n",
    "\n",
    "#Self defined coefficient to multiply with the reconstruction loss.\n",
    "CONFIG['reconstruction_coefficient'] = 1e-1  # 1e-2\n",
    "#Defines a stopping point for pre-training the encoder/decoder\n",
    "CONFIG['recon_threshold'] = 0.0005 \n",
    "CONFIG['projection_coefficient'] = 1e-1  # 1e-2\n",
    "\n",
    "#The width of the drug-drug pair's multi-hot functional representation\n",
    "CONFIG['input_dim'] = 1722\n",
    "CONFIG['num_class'] = 2\n",
    "\n",
    "#===== Encoder/Decoder Parameters =====\n",
    "CONFIG['encode_fc1_dim'] = 500  # encoder fc1\n",
    "CONFIG['encode_fc2_dim'] = 50  # encoder fc2\n",
    "CONFIG['decode_fc1_dim'] = 500  # decoder fc1\n",
    "CONFIG['decode_fc2_dim'] = CONFIG['input_dim']  # decoder reconstruction\n",
    "\n",
    "#===== Deep Predictor Parameters =====\n",
    "CONFIG['magnify_factor'] = 100\n",
    "CONFIG['predict_dim'] = 512 # for every layer\n",
    "CONFIG['predict_out_dim'] = 1 # predictor out\n",
    "\n",
    "CONFIG['lambda1'] = 1e-2  # L1 regularization coefficient\n",
    "CONFIG['lambda2'] = 1e-1  # L2 regulatization coefficient\n",
    "CONFIG['lambda3'] = 1e-5  # L2 regulatization coefficient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2NbPHUTMbkD3"
   },
   "source": [
    "##  Data:\n",
    "The data that CASTER ingests consists of 3 datasets, already integrated in the repo:\n",
    "  * unsup_dataset.csv = A dataset of randomly combined pairs of SMILEs strings drawn from FooDB (a db of food constituent molecules) and all drugs, drawn from DrugBank\n",
    "    * This unsupervised dataset is used to help find frequent SMILEs substructures and train the encoder/decoder\n",
    "  * BIOSNAP/sup*.csv = A dataset from Stanford's Biomedical Network Dataset indicating pairs of SMILEs strings and presence of DDI\n",
    "    * These are the supervised datasets for training the predictor\n",
    "  * subword_units_map.csv = The 1722 frequent patterns extracted from the unsupervised strings, already produced by the sequential pattern mining (SPM) routine\n",
    "\n",
    "The 2 cells below define and load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===== DATASET DEFINITIONS =====\n",
    "vocab_path = \"data/codes.txt\"\n",
    "bpe_codes_fin = codecs.open(vocab_path)\n",
    "bpe = BPE(bpe_codes_fin, merges=-1, separator='')\n",
    "\n",
    "#Get frequent substructures\n",
    "vocab_map = pd.read_csv('data/subword_units_map.csv')\n",
    "idx2word = vocab_map['index'].values\n",
    "words2idx = dict(zip(idx2word, range(0, len(idx2word))))\n",
    "\n",
    "#===== Helper functions =====\n",
    "#Map 2 smiles strings into multi-hot representations of their substructures.\n",
    "def smiles_2_index(s1, s2):\n",
    "    t1 = bpe.process_line(s1).split() #split\n",
    "    t2 = bpe.process_line(s2).split() #split\n",
    "    i1 = [words2idx[i] for i in t1] # index\n",
    "    i2 = [words2idx[i] for i in t2] # index\n",
    "    return i1, i2\n",
    "\n",
    "#Combines both multi-hot representations into a single multi-hot (the functional representation)\n",
    "def index_2_multi_hot(i1, i2):\n",
    "    v_d = np.zeros(len(idx2word),dtype=np.float32)\n",
    "    v_d[i1] = 1\n",
    "    v_d[i2] = 1\n",
    "    return v_d\n",
    "\n",
    "#Combination of above: Takes two smiles strings and turns them into the multi-hot functional representation.\n",
    "def smiles_2_vector(s1, s2):\n",
    "    i1, i2 = smiles_2_index(s1, s2)\n",
    "    v_d = index_2_multi_hot(i1, i2)\n",
    "    return v_d\n",
    "\n",
    "#===== Datasets =====\n",
    "class sup_data(data.Dataset):\n",
    "\n",
    "    def __init__(self, list_IDs, labels, df_ddi):\n",
    "        'Initialization'\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.df = df_ddi\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.list_IDs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        index = self.list_IDs[index]\n",
    "        # Load data from sample and get multi-hot\n",
    "        s1 = self.df.iloc[index].Drug1_SMILES\n",
    "        s2 = self.df.iloc[index].Drug2_SMILES\n",
    "        v_d = smiles_2_vector(s1, s2)\n",
    "        #Get label\n",
    "        y = self.labels[index]\n",
    "        #Return combined multi-hot and its label.\n",
    "        return v_d, y\n",
    "    \n",
    "class unsup_data(data.Dataset):\n",
    "\n",
    "    def __init__(self, list_IDs, df):\n",
    "        'Initialization'\n",
    "        self.list_IDs = list_IDs\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.list_IDs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Load data and get label\n",
    "        index = self.list_IDs[index]\n",
    "        s1 = self.df.iloc[index].input1_SMILES\n",
    "        s2 = self.df.iloc[index].input2_SMILES\n",
    "        v_d = smiles_2_vector(s1, s2)\n",
    "        return v_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "BZScZNbROw-N"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "TOTAL UNSUPERVISED SET\n",
      "==============================\n",
      "# of samples: 441853\n",
      "# of drug-food pairs: 220926\n",
      "# of drug-drug pairs: 220927\n",
      "\n",
      "\n",
      "==============================\n",
      "BIOSNAP TRAIN/VAL SET\n",
      "==============================\n",
      "# of samples: 66432\n",
      "# of DDIs: 33243\n"
     ]
    }
   ],
   "source": [
    "#Read datasets\n",
    "df_unsup = pd.read_csv('data/unsup_dataset.csv', \n",
    "                       names = ['idx', 'input1_SMILES', 'input2_SMILES', 'type']).drop(0)# pairs dataframe input1_smiles, input2_smiles\n",
    "df_ddi = pd.read_csv('data/BIOSNAP/sup_train_val.csv')  # ddi dataframe drug1_smiles, drug2_smiles\n",
    "df_ddi.label = df_ddi.label.astype('float32')\n",
    "\n",
    "#Print some basic info about the datasets \n",
    "print(\"=\"*30)\n",
    "print(\"TOTAL UNSUPERVISED SET\")\n",
    "print(\"=\"*30)\n",
    "print(\"# of samples:\",len(df_unsup))\n",
    "print(\"# of drug-food pairs:\",sum(df_unsup.type == \"df_pair\"))\n",
    "print(\"# of drug-drug pairs:\",sum(df_unsup.type == \"dd_pair\"))\n",
    "print(\"\\n\")\n",
    "print(\"=\"*30)\n",
    "print(\"BIOSNAP TRAIN/VAL SET\")\n",
    "print(\"=\"*30)\n",
    "print(\"# of samples:\",len(df_ddi))\n",
    "print(\"# of DDIs:\",sum(df_ddi.label == 1.0))\n",
    "\n",
    "#K-fold cross-validation.\n",
    "kf = KFold(n_splits = 8, shuffle = True, random_state = 3)\n",
    "\n",
    "#Get the 1st fold index\n",
    "fold_index = next(kf.split(df_ddi), None)\n",
    "\n",
    "ids_unsup = df_unsup.index.values\n",
    "partition_sup = {'train': fold_index[0], 'val': fold_index[1]}\n",
    "labels_sup = df_ddi.label.values\n",
    "\n",
    "LOADER_PARAMS = {\n",
    "    'batch_size': CONFIG[\"batch_size\"],\n",
    "    'shuffle': True,\n",
    "    'num_workers': 0\n",
    "    }\n",
    "\n",
    "unsup_set = unsup_data(ids_unsup, df_unsup)\n",
    "unsup_generator = data.DataLoader(unsup_set, **LOADER_PARAMS)\n",
    "\n",
    "training_set = sup_data(partition_sup['train'], labels_sup, df_ddi)\n",
    "training_generator_sup = data.DataLoader(training_set, **LOADER_PARAMS)\n",
    "\n",
    "validation_set = sup_data(partition_sup['val'], labels_sup, df_ddi)\n",
    "validation_generator_sup = data.DataLoader(validation_set, **LOADER_PARAMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3muyDPFPbozY"
   },
   "source": [
    "## Model Definition:\n",
    "\n",
    "Based on CASTER's github, the model was defined with the following parameters:\n",
    "* Encoder/Decoder: 2 Layer 500x50 NNs with ReLU activations\n",
    "* Predictor: 7 Layer 1024 wide NNs with ReLU activations\n",
    "* Optimizer: Adam\n",
    "\n",
    "## **TODO: Expand**\n",
    "\n",
    "The cell below defines and instantiates the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "gBdVZoTvsSFV"
   },
   "outputs": [],
   "source": [
    "class CASTER(nn.Sequential):\n",
    "\n",
    "    def __init__(self, **config):\n",
    "        super(CASTER, self).__init__()\n",
    "        self.input_dim = config['input_dim']\n",
    "        self.num_class = config['num_class']\n",
    "        self.lambda3 = config['lambda3']        \n",
    "        self.encode_fc1_dim = config['encode_fc1_dim']\n",
    "        self.encode_fc2_dim = config['encode_fc2_dim']\n",
    "        self.decode_fc1_dim = config['decode_fc1_dim']\n",
    "        self.decode_fc2_dim = config['decode_fc2_dim']\n",
    "        self.predict_dim = config['predict_dim']\n",
    "        self.predict_out_dim = config['predict_out_dim']\n",
    "        self.mag_factor = config['magnify_factor']        \n",
    "\n",
    "        # encoder: two layer NN\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, self.encode_fc1_dim),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(self.encode_fc1_dim, self.encode_fc2_dim)\n",
    "        )\n",
    "        \n",
    "        # decoder: two layer NN\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.encode_fc2_dim, self.decode_fc1_dim),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(self.decode_fc1_dim, self.decode_fc2_dim)\n",
    "        )\n",
    "\n",
    "        # predictor: eight layer NN\n",
    "        self.predictor = nn.Sequential(\n",
    "            # layer 1\n",
    "            nn.Linear(self.input_dim, self.predict_dim),\n",
    "            nn.ReLU(True),\n",
    "            # layer 2\n",
    "            nn.BatchNorm1d(self.predict_dim),\n",
    "            nn.Linear(self.predict_dim, self.predict_dim),\n",
    "            nn.ReLU(True),\n",
    "            # layer 3\n",
    "            nn.BatchNorm1d(self.predict_dim),\n",
    "            nn.Linear(self.predict_dim, self.predict_dim),\n",
    "            nn.ReLU(True),\n",
    "            # layer 4\n",
    "            nn.BatchNorm1d(self.predict_dim),\n",
    "            nn.Linear(self.predict_dim, self.predict_dim),\n",
    "            nn.ReLU(True),\n",
    "            # layer 5\n",
    "            nn.BatchNorm1d(self.predict_dim),\n",
    "            nn.Linear(self.predict_dim, self.predict_dim),\n",
    "            nn.ReLU(True),\n",
    "            # layer 6\n",
    "            nn.BatchNorm1d(self.predict_dim),\n",
    "            nn.Linear(self.predict_dim, 64),\n",
    "            nn.ReLU(True),\n",
    "            # output layer\n",
    "            nn.Linear(64, self.predict_out_dim)\n",
    "        )\n",
    "\n",
    "    def dictionary_encoder(self, z, latent_dict):\n",
    "        '''\n",
    "        :param z: batch_size x encode_fc2_dim\n",
    "        :param latent_dict: encode_fc2_dim x eta\n",
    "        :return: sparse code r: batch_size x eta\n",
    "        '''       \n",
    "        \n",
    "        DTD = torch.matmul(latent_dict, latent_dict.transpose(2, 1))  # D is Dictionary;  D^T D encode_dim x eta\n",
    "        temp = DTD + self.lambda3 * torch.eye(self.input_dim)\n",
    "        DTD_inv = torch.inverse(temp)  # (D^T D + \\lambda2 I )^{-1} D^T D, eta x eta\n",
    "        DTD_inv_DT = torch.matmul(DTD_inv, latent_dict)  \n",
    "\n",
    "        r = z[:,None,:].matmul(DTD_inv_DT.transpose(2, 1)).squeeze(1) # batch_size x eta    \n",
    "        return r\n",
    "\n",
    "    def forward(self, x_vec):\n",
    "        '''\n",
    "        :param x_vec: batch_size x width of multi-hot functional\n",
    "        :return: recon, r_vec, score\n",
    "        '''\n",
    "        _, x_width = x_vec.shape\n",
    "        \n",
    "        # Encode functional representation into latent representation \n",
    "        z_vec = self.encoder(x_vec)\n",
    "        \n",
    "        # Create latent dictionary using encoder\n",
    "        latent_dict = self.encoder(torch.eye(x_width))\n",
    "        latent_dict = latent_dict.mul(x_vec[:,:,None]) \n",
    "        \n",
    "        # Use dictionary to get coeffs/r_vec\n",
    "        r_vec = self.dictionary_encoder(z_vec, latent_dict)\n",
    "\n",
    "        # Pass coeffs through the deep predictor\n",
    "        score = self.predictor(self.mag_factor * r_vec)\n",
    "\n",
    "        # Decode latent representation\n",
    "        recon_temp = self.decoder(z_vec)\n",
    "        reconstructed = torch.sigmoid(recon_temp)\n",
    "        \n",
    "        # print(reconstructed, r_vec, score, latent_dict, z_vec)\n",
    "        return reconstructed, r_vec, score, latent_dict, z_vec\n",
    "\n",
    "#Instantiated model\n",
    "caster_model = CASTER(**CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training:\n",
    "Training proceeds in two phases. \n",
    "1. **Pre-training:**\n",
    "    * This phase is focused on getting the autoencoder to find the most efficient latent representation for the functional representation. Note that the classification loss is excluded in the phase, so the autoencoder gets a \"head start\".\n",
    "2. **Training:**\n",
    "    * This phase trains the predictor with the linear coefficients (combination of latent dictionary w/ latent representation). Classification loss is added back into this phase.\n",
    "\n",
    "### Parameters:\n",
    "According to the paper, the \n",
    "# FILL OUT\n",
    "\n",
    "### Notes on Hardware and Runtime:\n",
    "The workstation this notebook was run on contains a Ryzen 7 5700x CPU and 48 GB of RAM. We did not utilize any CUDA enabled GPU.\n",
    "\n",
    "On Windows, the runtimes were:\n",
    "* Pre-Training & Training : ~50 sec / iteration\n",
    "* Evaluation : ~12 sec / iteration, ~70 sec total\n",
    "\n",
    "However, using WSL (Ubuntu 22.04), runtimes were:\n",
    "* Pre-Training & Training : ~13.5 sec / iteration\n",
    "* Evaluation : ~6 sec / iteration, ~70 sec total\n",
    "\n",
    "Performance profiling the script revealed that the slowest operation was the matrix inverse operation in the dictionary encoder.\n",
    "\n",
    "In total, it is well below the 8 min runtime limit. Depending on available compute power, your expected runtime may increase or decrease. For the sake of convenience, a `QUICK_TRAIN_DEMO` parameter was added into the `CONFIG` cell to allow for a quick demo, as its name implies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Misc loss histories\n",
    "loss_r_history = []\n",
    "loss_p_history = []\n",
    "loss_c_history = []\n",
    "loss_history = []\n",
    "\n",
    "#How many epochs to devote to training the encoder/decoder\n",
    "PRETRAIN_EPOCHS = CONFIG['pretrain_epochs']\n",
    "#Defines an early stopping point for pre-training the encoder/decoder\n",
    "RECON_THRESH = CONFIG['recon_threshold']\n",
    "#Self defined coefficient to multiply with the reconstruction loss.\n",
    "RECON_LOSS_COEFF = CONFIG['reconstruction_coefficient']\n",
    "\n",
    "TRAIN_EPOCHS = CONFIG[\"train_epochs\"]\n",
    "PROJ_COEFF = CONFIG['projection_coefficient']\n",
    "LAM1 = CONFIG['lambda1']\n",
    "LAM2 = CONFIG['lambda2']\n",
    "\n",
    "LR = CONFIG['LR']\n",
    "BATCH_SIZE = CONFIG[\"batch_size\"]\n",
    "opt = torch.optim.Adam(caster_model.parameters(), lr = LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUICK DEMO ACTIVE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:13, 13.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Training at Epoch 0 iteration 0, total loss is 4.105, proj loss is 4.036, recon loss is 0.069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:26, 13.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Training at Epoch 0 iteration 1, total loss is 3.040, proj loss is 2.971, recon loss is 0.069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:39, 13.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Training at Epoch 0 iteration 2, total loss is 2.703, proj loss is 2.635, recon loss is 0.068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:52, 13.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Training at Epoch 0 iteration 3, total loss is 2.325, proj loss is 2.258, recon loss is 0.067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [01:06, 13.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Training at Epoch 0 iteration 4, total loss is 2.309, proj loss is 2.243, recon loss is 0.066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [01:19, 13.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Training at Epoch 0 iteration 5, total loss is 2.232, proj loss is 2.167, recon loss is 0.065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [01:32, 13.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Training at Epoch 0 iteration 6, total loss is 2.200, proj loss is 2.136, recon loss is 0.064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [01:46, 13.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Training at Epoch 0 iteration 7, total loss is 2.075, proj loss is 2.012, recon loss is 0.063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [01:59, 13.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Training at Epoch 0 iteration 8, total loss is 1.832, proj loss is 1.770, recon loss is 0.062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [02:13, 13.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Training at Epoch 0 iteration 9, total loss is 1.706, proj loss is 1.645, recon loss is 0.061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [02:26, 13.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Training at Epoch 0 iteration 10, total loss is 1.655, proj loss is 1.595, recon loss is 0.060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [02:40, 13.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Training at Epoch 0 iteration 11, total loss is 1.540, proj loss is 1.481, recon loss is 0.059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [02:54, 13.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Training at Epoch 0 iteration 12, total loss is 1.475, proj loss is 1.417, recon loss is 0.058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [03:08, 13.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Training at Epoch 0 iteration 13, total loss is 1.440, proj loss is 1.382, recon loss is 0.057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [03:21, 13.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Training at Epoch 0 iteration 14, total loss is 1.340, proj loss is 1.283, recon loss is 0.056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [03:35, 13.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Training at Epoch 0 iteration 15, total loss is 1.324, proj loss is 1.268, recon loss is 0.055\n"
     ]
    }
   ],
   "source": [
    "#===== PRE-TRAINING =====\n",
    "len_unsup = len(unsup_generator)\n",
    "\n",
    "if QUICK_TRAIN_DEMO:\n",
    "    print(\"QUICK DEMO ACTIVE\")\n",
    "\n",
    "for pre_epo in range(PRETRAIN_EPOCHS):\n",
    "\n",
    "    #Use unsupervised datasat to train encoder/decoder\n",
    "    for iter_idx, v_D in tqdm(enumerate(unsup_generator)):\n",
    "        v_D = v_D.float()\n",
    "\n",
    "        recon, code, score, Z_f, z_D = caster_model(v_D)\n",
    "        \n",
    "        loss_r = RECON_LOSS_COEFF * F.binary_cross_entropy(recon, v_D)\n",
    "        \n",
    "        loss_p = PROJ_COEFF * (torch.norm(z_D - torch.matmul(code, Z_f)) + \\\n",
    "                LAM1 * torch.sum(torch.abs(code)) / BATCH_SIZE + \\\n",
    "                LAM2 * torch.norm(Z_f, p='fro') / BATCH_SIZE)\n",
    "        \n",
    "        total_loss = loss_r + loss_p\n",
    "        \n",
    "        loss_r_history.append(loss_r.detach())\n",
    "        loss_p_history.append(loss_p.detach())\n",
    "        loss_history.append(total_loss.detach())\n",
    "\n",
    "        opt.zero_grad()\n",
    "        total_loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        # if(i % 10 == 0):\n",
    "        print('Pre-Training at Epoch ' + str(pre_epo) + \\\n",
    "                ' iteration ' + str(iter_idx) + \\\n",
    "                ', total loss is ' + '%.3f' % (total_loss.cpu().detach().numpy()) + \\\n",
    "                ', proj loss is ' + '%.3f' % (loss_p.cpu().detach().numpy()) + \\\n",
    "                ', recon loss is ' + '%.3f' % (loss_r.cpu().detach().numpy()))\n",
    "        \n",
    "        # save pretraining checkpoint for reuse\n",
    "        if iter_idx == int(len_unsup/4):\n",
    "            torch.save(caster_model, 'model_pretrain_checkpoint_1.pt')\n",
    "        if iter_idx == int(len_unsup/2):\n",
    "            torch.save(caster_model, 'model_pretrain_checkpoint_1.pt')\n",
    "\n",
    "        #If the reconstruction loss is smaller than the specified threshold, finish the pre-training and go train.\n",
    "        if QUICK_TRAIN_DEMO and iter_idx >= 64 or loss_r < RECON_THRESH:\n",
    "            break\n",
    "        \n",
    "    if QUICK_TRAIN_DEMO:\n",
    "        break\n",
    "print(\"Done with pre-training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUICK DEMO ACTIVE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:13, 13.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training at Epoch 0 iteration 0, total loss is 1.947, proj loss is 1.201, recon loss is 0.053, classification loss is 0.693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:27, 13.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training at Epoch 0 iteration 1, total loss is 1.881, proj loss is 1.141, recon loss is 0.052, classification loss is 0.687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:41, 13.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training at Epoch 0 iteration 2, total loss is 1.825, proj loss is 1.119, recon loss is 0.051, classification loss is 0.655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:55, 13.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training at Epoch 0 iteration 3, total loss is 1.770, proj loss is 1.032, recon loss is 0.050, classification loss is 0.688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [01:08, 13.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training at Epoch 0 iteration 4, total loss is 1.758, proj loss is 1.019, recon loss is 0.049, classification loss is 0.690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [01:22, 13.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training at Epoch 0 iteration 5, total loss is 1.703, proj loss is 0.988, recon loss is 0.048, classification loss is 0.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [01:36, 13.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training at Epoch 0 iteration 6, total loss is 1.629, proj loss is 0.930, recon loss is 0.047, classification loss is 0.652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [01:50, 13.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training at Epoch 0 iteration 7, total loss is 1.689, proj loss is 0.940, recon loss is 0.046, classification loss is 0.703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [02:04, 13.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training at Epoch 0 iteration 8, total loss is 1.607, proj loss is 0.891, recon loss is 0.045, classification loss is 0.671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [02:17, 13.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training at Epoch 0 iteration 9, total loss is 1.618, proj loss is 0.873, recon loss is 0.044, classification loss is 0.701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [02:31, 13.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training at Epoch 0 iteration 10, total loss is 1.518, proj loss is 0.849, recon loss is 0.042, classification loss is 0.627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [02:45, 13.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training at Epoch 0 iteration 11, total loss is 1.572, proj loss is 0.849, recon loss is 0.041, classification loss is 0.682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [02:59, 13.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training at Epoch 0 iteration 12, total loss is 1.535, proj loss is 0.863, recon loss is 0.040, classification loss is 0.632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [03:13, 13.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training at Epoch 0 iteration 13, total loss is 1.481, proj loss is 0.820, recon loss is 0.039, classification loss is 0.621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [03:26, 13.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training at Epoch 0 iteration 14, total loss is 1.497, proj loss is 0.825, recon loss is 0.038, classification loss is 0.634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [03:40, 13.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training at Epoch 0 iteration 15, total loss is 1.503, proj loss is 0.822, recon loss is 0.037, classification loss is 0.643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [03:54, 14.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training at Epoch 0 iteration 16, total loss is 1.414, proj loss is 0.792, recon loss is 0.036, classification loss is 0.585\n",
      "Done with training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#===== TRAINING =====\n",
    "\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "bce_loss = torch.nn.BCELoss()\n",
    "\n",
    "if QUICK_TRAIN_DEMO:\n",
    "    print(\"QUICK DEMO ACTIVE\")\n",
    "\n",
    "for tr_epo in range(TRAIN_EPOCHS):\n",
    "    for iter_idx, (v_D, label) in tqdm(enumerate(training_generator_sup)):\n",
    "        v_D = v_D.float()\n",
    "        recon, code, score, Z_f, z_D = caster_model(v_D)\n",
    "        label = Variable(torch.from_numpy(np.array(label)).long())\n",
    "        n = torch.squeeze(sigmoid(score))\n",
    "        \n",
    "        loss_c = bce_loss(n, label.float())\n",
    "        loss_r = RECON_LOSS_COEFF * F.binary_cross_entropy(recon, v_D)\n",
    "        \n",
    "        loss_p =    PROJ_COEFF * (torch.norm(z_D - torch.matmul(code, Z_f)) + \\\n",
    "                    LAM1 * torch.sum(torch.abs(code)) / BATCH_SIZE + \\\n",
    "                    LAM2 * torch.norm(Z_f, p='fro') / BATCH_SIZE)\n",
    "        \n",
    "        total_loss = loss_c + loss_r + loss_p\n",
    "        loss_r_history.append(loss_r.detach())\n",
    "        loss_p_history.append(loss_p.detach())\n",
    "        loss_c_history.append(loss_c.detach())\n",
    "        loss_history.append(total_loss.detach())\n",
    "\n",
    "        opt.zero_grad()\n",
    "        total_loss.backward()\n",
    "        opt.step()\n",
    "                \n",
    "        print(  'Training at Epoch ' + str(tr_epo) + \\\n",
    "                ' iteration ' + str(iter_idx) + \\\n",
    "                ', total loss is ' + '%.3f' % (total_loss.detach().numpy()) + \\\n",
    "                ', proj loss is ' + '%.3f' % (loss_p.detach().numpy()) + \\\n",
    "                ', recon loss is ' + '%.3f' % (loss_r.detach().numpy()) + \\\n",
    "                ', classification loss is ' + '%.3f' % (loss_c.detach().numpy()))\n",
    "        \n",
    "        if QUICK_TRAIN_DEMO and iter_idx >= 64:\n",
    "            break\n",
    "    if QUICK_TRAIN_DEMO:\n",
    "        break\n",
    "    \n",
    "#Save the final model.\n",
    "torch.save(caster_model, 'model_nn_trained.pt')\n",
    "print(\"Done with training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablation Model: Logistic Regression\n",
    "\n",
    "One of the ablations we proposed (and one that was also suggested in the paper) was using logistic regression (LR) instead of CASTER's deep predictor. Specifically, the pair's functional representation (sub-structured/post pattern-mined) is classified using LR. According to the paper, though far lighter in parameter count, it was not chosen due its weaker performance. For brevity, definition and training have been combined into the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log reg for 36 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/36 [00:04<02:48,  4.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0. Loss: 0.6864171028137207.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 9/36 [00:43<02:09,  4.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8. Loss: 0.662103533744812.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 17/36 [01:21<01:30,  4.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16. Loss: 0.581276535987854.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 25/36 [01:59<00:52,  4.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24. Loss: 0.5914052128791809.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 33/36 [02:37<00:14,  4.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32. Loss: 0.5873168706893921.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [02:51<00:00,  4.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training LR.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class simple_log_reg(nn.Module):\n",
    "\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        super(simple_log_reg, self).__init__()\n",
    "        self.linear = torch.nn.Linear(n_inputs, n_outputs)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        pred = torch.sigmoid(self.linear(x))\n",
    "        return pred[:,-1]\n",
    "\n",
    "n_inputs = CONFIG[\"input_dim\"]\n",
    "n_outputs = 1\n",
    "\n",
    "#Instantiate log reg classifier\n",
    "log_reg_model = simple_log_reg(n_inputs, n_outputs)\n",
    "\n",
    "#Using SGD for simplicity\n",
    "log_reg_optimizer = torch.optim.SGD(log_reg_model.parameters(), lr=0.001)\n",
    "log_reg_criterion = torch.nn.BCELoss()\n",
    "\n",
    "epochs = 36\n",
    "print(\"Training log reg for {} epochs\".format(epochs))\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    for x_vec, y in training_generator_sup:\n",
    "        log_reg_optimizer.zero_grad()\n",
    "        log_reg_y_pred = log_reg_model(x_vec)\n",
    "        total_loss = log_reg_criterion(log_reg_y_pred, y)\n",
    "        total_loss.backward()\n",
    "        log_reg_optimizer.step()\n",
    "    if epoch % 8 == 0:\n",
    "        print('Epoch: {}. Loss: {}.'.format(epoch, total_loss.item()))\n",
    "print(\"Done training LR.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gX6bCcZNuxmz"
   },
   "source": [
    "# Results:\n",
    "---\n",
    "To begin the evaluation process, we iterate through a test data set using a data loader to obtain the predictions (y_pred) from the model for each input (v_D). We then use the ground truth labels (y_label) to which we compare our predictions to calculate the evaluation metrics.\n",
    "\n",
    "The evaluation metrics used here include some of the most commonly used metrics to assess the performance of binary models: \n",
    "\n",
    "* **Average Precision Score**: Measures the precision-recall trade-off by summarizing the precision-recall curve as a weighted mean of precisions achieved at various thresholds.\n",
    "\n",
    "* **Area under the ROC Curve (AUC)**: This metric effectively describes the trade-off between true-positive rate and false-positive rate. It essentially quantifies the ability of the model to distingush between classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LjW9bCkouv8O"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'QUICK_TRAIN_DEMO' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m log_reg_model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     22\u001b[0m sigmoid \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mSigmoid()\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mQUICK_TRAIN_DEMO\u001b[49m:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQUICK DEMO ACTIVE\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iter_idx, (v_D, label) \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(test_generator_sup)):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'QUICK_TRAIN_DEMO' is not defined"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "        'batch_size': 128,\n",
    "        'shuffle': True,\n",
    "        'num_workers': 0\n",
    "        }\n",
    "\n",
    "test_set = sup_data(df_ddi.index.values, labels_sup, df_ddi)\n",
    "test_generator_sup = data.DataLoader(test_set, **params)\n",
    "\n",
    "log_reg_y_pred = []\n",
    "log_reg_y_label = []\n",
    "\n",
    "caster_y_pred = []\n",
    "caster_y_label = []\n",
    "\n",
    "#Set caster model to eval\n",
    "caster_model.eval()\n",
    "\n",
    "#Set log reg to eval\n",
    "log_reg_model.eval()\n",
    "\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "if QUICK_TRAIN_DEMO:\n",
    "    print(\"QUICK DEMO ACTIVE\")\n",
    "\n",
    "for iter_idx, (v_D, label) in tqdm(enumerate(test_generator_sup)):\n",
    "    label_ids = label.to('cpu').numpy()\n",
    "\n",
    "    recon, r_vec, score, latent_dict, z_D = caster_model(v_D.float())\n",
    "    logits = torch.squeeze(sigmoid(score)).detach().cpu().numpy()\n",
    "    caster_y_label = caster_y_label + label_ids.flatten().tolist()\n",
    "    caster_y_pred = caster_y_pred + logits.flatten().tolist()\n",
    "\n",
    "    lr_outputs = log_reg_model(v_D.float())\n",
    "    log_reg_y_label = log_reg_y_label + label_ids.flatten().tolist()\n",
    "    log_reg_y_pred = log_reg_y_pred + lr_outputs.flatten().tolist()\n",
    "    \n",
    "    #Stop early at 16 iterations if QUICK_DEMO\n",
    "    if QUICK_TRAIN_DEMO and iter_idx > 16:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cells, we will plot the precision-recall curve and the AUC described above. The precision-recall curve demonstrates the precision of the model at various recall levels. This gives us an idea of the model's ability to identify positive occurences with false positives minimized.\n",
    "\n",
    "Overall, using these metrics and evaluations, we get a clear picture of the model's performance across different thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7593552553269254"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "\n",
    "average_precision = average_precision_score(caster_y_label, caster_y_pred)\n",
    "precision, recall, _ = precision_recall_curve(caster_y_label, caster_y_pred)\n",
    "\n",
    "plt.step(recall, precision, color='b', alpha=0.2,\n",
    "         where='post')\n",
    "plt.fill_between(recall, precision, alpha=0.2, color='b')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(\n",
    "          average_precision))\n",
    "average_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [2304, 16]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m caster_fpr, caster_tpr, thresholds \u001b[38;5;241m=\u001b[39m roc_curve(caster_y_label, caster_y_pred)\n\u001b[1;32m      6\u001b[0m caster_auc_score \u001b[38;5;241m=\u001b[39m auc(caster_fpr, caster_tpr)\n\u001b[0;32m----> 8\u001b[0m log_reg_fpr, log_reg_tpr, lr_thresholds \u001b[38;5;241m=\u001b[39m \u001b[43mroc_curve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_reg_y_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_reg_y_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m log_reg_auc_score \u001b[38;5;241m=\u001b[39m auc(log_reg_fpr, log_reg_tpr)\n\u001b[1;32m     11\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:1108\u001b[0m, in \u001b[0;36mroc_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[1;32m   1007\u001b[0m     {\n\u001b[1;32m   1008\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1017\u001b[0m     y_true, y_score, \u001b[38;5;241m*\u001b[39m, pos_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, drop_intermediate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m ):\n\u001b[1;32m   1019\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute Receiver operating characteristic (ROC).\u001b[39;00m\n\u001b[1;32m   1020\u001b[0m \n\u001b[1;32m   1021\u001b[0m \u001b[38;5;124;03m    Note: this implementation is restricted to the binary classification task.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;124;03m    array([ inf, 0.8 , 0.4 , 0.35, 0.1 ])\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1108\u001b[0m     fps, tps, thresholds \u001b[38;5;241m=\u001b[39m \u001b[43m_binary_clf_curve\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1112\u001b[0m     \u001b[38;5;66;03m# Attempt to drop thresholds corresponding to points in between and\u001b[39;00m\n\u001b[1;32m   1113\u001b[0m     \u001b[38;5;66;03m# collinear with other points. These are always suboptimal and do not\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m     \u001b[38;5;66;03m# appear on a plotted ROC curve (and thus do not affect the AUC).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1119\u001b[0m     \u001b[38;5;66;03m# but does not drop more complicated cases like fps = [1, 3, 7],\u001b[39;00m\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;66;03m# tps = [1, 2, 4]; there is no harm in keeping too many thresholds.\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m drop_intermediate \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fps) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:819\u001b[0m, in \u001b[0;36m_binary_clf_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m pos_label \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)):\n\u001b[1;32m    817\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m format is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(y_type))\n\u001b[0;32m--> 819\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    820\u001b[0m y_true \u001b[38;5;241m=\u001b[39m column_or_1d(y_true)\n\u001b[1;32m    821\u001b[0m y_score \u001b[38;5;241m=\u001b[39m column_or_1d(y_score)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:457\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    455\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    458\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    459\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[1;32m    460\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [2304, 16]"
     ]
    }
   ],
   "source": [
    "#===== ROC CURVE CODE =====\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, roc_curve, auc, confusion_matrix, classification_report\n",
    "\n",
    "caster_fpr, caster_tpr, thresholds = roc_curve(caster_y_label, caster_y_pred)\n",
    "caster_auc_score = auc(caster_fpr, caster_tpr)\n",
    "\n",
    "log_reg_fpr, log_reg_tpr, lr_thresholds = roc_curve(log_reg_y_label, log_reg_y_pred)\n",
    "log_reg_auc_score = auc(log_reg_fpr, log_reg_tpr)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(caster_fpr, caster_tpr, label='CASTER (area = {:.3f})'.format(caster_auc_score))\n",
    "plt.plot(log_reg_fpr, log_reg_tpr, label='LR (area = {:.3f})'.format(log_reg_auc_score))\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.title('ROC')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8EAWAy_LwHlV"
   },
   "source": [
    "# Model Comparison:\n",
    "\n",
    "---\n",
    "\n",
    "*TODO*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qH75TNU71eRH"
   },
   "source": [
    "# Discussion :\n",
    "---\n",
    "\n",
    "* Is the paper reproducible?\n",
    "    * **Yes.** At this point, we’ve demonstrated that we can instantiate, train, and evaluate the model, thus we believe that it is, indeed, reproducible. The only blocker is our lack of compute power. At ~50 sec/iteration for training on a beefier CPU (due to lack of an RTX 4090 / other CUDA GPU), it would be untenable to train both phases on our local machines. Our plan moving forward is to secure the appropriate compute resources from Google Cloud and have our model run within a reasonable timeframe. The authors did mention their hardware specifications but did not mention a timeframe for how long they trained for. We can reduce model size and depth accordingly, if cloud compute resources would still not be enough or we run out of credits.\n",
    "\n",
    "* What was easy?\n",
    "    * One of the aspects of the paper reproduction that we found straightforward was understanding the authors’ intentions in the code. Although the script was not accompanied by copious documentation, the comments in the code, a relatively clear procedural flow in the script, and slowly working through the paper helped us understand what is being accomplished with the provided code.\n",
    "    \n",
    "* What was difficult?\n",
    "    * One aspect that we found difficult was finding the right computing power to handle the scale of computation demanded by the CASTER model. In the subsequent project phase, we plan to migrate our replication efforts to cloud infrastructure thereby availing ourselves of improved computational abilities.\n",
    "\n",
    "* Future Plans:\n",
    "    * In the next phase, we plan on either migrating our notebook to Google Colab (this free version of Colab grants access to Nvidia's T4 GPUs subject to quota restrictions and availability), Google Colab Pro (which grants access to premium V100 or A100 Nvidia GPUs), Google Cloud Platform, or Kaggle (which grants access to two T4 GPUs and one A100 GPU subject to quota restrictions and availability as well). Depending on the model parameter sizes and training time, we will use one or more of these platforms, possibly in parallel. We will also finish our planned ablations, refactor, and add more detailed comments for a more readable and easily understandable notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHMI2chl9omn"
   },
   "source": [
    "# References\n",
    "\n",
    "1.   Huang, K., Xiao, C., Hoang, T., Glass, L., & Sun, J. (2020, April). Caster: Predicting drug interactions with chemical substructure representation. In Proceedings of the AAAI conference on artificial intelligence (Vol. 34, No. 01, pp. 702-709).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1oAKqszNlwEZwPa_BjHPqfcoWlikYBpi5",
     "timestamp": 1709153069464
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
